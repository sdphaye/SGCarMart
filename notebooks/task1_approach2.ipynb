{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from cleaner import clean_preliminary, to_categorical_for_cols, remove_nominal_cols, clean_sim_filled_data\n",
    "from predictor import fill_ml_na\n",
    "import constants as const\n",
    "from utils import round_to_nearest_hundred, preds_to_csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading and Preparing Train Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series of functions applied to the training data prepare the data as described in Approach 2 in our report. We briefly describe the use case of each funtion below: <br> \n",
    "- *clean_preliminary()*:\n",
    "    - Handles each columns individually\n",
    "    - Drops the unwanted rows and columns\n",
    "- *to_categorical_for_cols()*:\n",
    "    - Encodes *fuel_type* using dummy variables and *category* using MultiLabelBinarizer()\n",
    "- *remove_nominal_cols()*:\n",
    "    - Removes columns that are not needed for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(const.TRAIN_PATH)\n",
    "test = pd.read_csv(const.TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = clean_preliminary(df)\n",
    "df = to_categorical_for_cols(df)\n",
    "df = remove_nominal_cols(df)\n",
    "\n",
    "test = clean_preliminary(test, is_test=True)\n",
    "test = to_categorical_for_cols(test)\n",
    "test = remove_nominal_cols(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filling Null Values\n",
    "We have two ways to fill the null values. The training of both these processes take time, further, the saved ML models are very heavy to be pushed on the GitHub repository. We therefore save the data after applying the necessary approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Machine Learning Based Approach\n",
    "The function *fill_ml_na()* is used to train Machine learning models, to predict the missing values for each column. In practice, we use RandomizedSearchCV with 200 iterations and 5-fold cross validation. <br>\n",
    "Since the trained models are heavy, we do not include them in the repository, you can run the below code cell where we have set the number of iterations as 1 and k as 2 for cross validation to allow a quick sample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_train = fill_ml_na(df,training=True,num_iter=1,k_splits=2)\n",
    "# filled_train = fill_ml_na(df)\n",
    "filled_test = fill_ml_na(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, you can use the below code cell to access the training data we have saved after filling null values using the ML models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_train = pd.read_csv(const.ML_REPLACED_TRAIN)\n",
    "filled_test = pd.read_csv(const.ML_REPLACED_TEST)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Similarity Based Approach\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generate the similarity filled data from scratch by running the `generate_sim_df.py` file.\n",
    "\n",
    "Alternatively, you can use the below code cell to access the training data we have saved after filling null values using the similarity approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_train = pd.read_csv(const.SIM_REPLACED_TRAIN)\n",
    "filled_test = pd.read_csv(const.SIM_REPLACED_TEST)\n",
    "\n",
    "filled_train = clean_sim_filled_data(filled_train)\n",
    "filled_test = clean_sim_filled_data(filled_test, is_test=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br> \n",
    "\n",
    "___Since the data is handled differently by the two regressors, please relaod the data from above after you are done using one regressor.___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Model using GradientBoostingRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "filled_train.dropna(inplace=True)\n",
    "filled_test.fillna(filled_test.median(),inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filled_train.drop(['index','price'],axis=1)\n",
    "Y = filled_train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = GradientBoostingRegressor()\n",
    "model.fit(X_train,y_train)\n",
    "pred = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "27012.532713076296"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(y_test,pred,squared=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train LightGBM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = filled_train.drop(['index','price'],axis=1)\n",
    "Y = filled_train.price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kanav7/anaconda3/lib/python3.8/site-packages/lightgbm/engine.py:148: UserWarning: Found `num_iterations` in params. Will use it instead of argument\n",
      "  _log_warning(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LightGBM] [Warning] bagging_freq is set=100, subsample_freq=0 will be ignored. Current value: bagging_freq=100\n",
      "[LightGBM] [Warning] feature_fraction is set=0.6, colsample_bytree=1.0 will be ignored. Current value: feature_fraction=0.6\n",
      "[LightGBM] [Warning] bagging_fraction is set=1, subsample=1.0 will be ignored. Current value: bagging_fraction=1\n",
      "[LightGBM] [Warning] min_data_in_leaf is set=7, min_child_samples=20 will be ignored. Current value: min_data_in_leaf=7\n",
      " RMSE: 28105.328831\n"
     ]
    }
   ],
   "source": [
    "model = lgb.LGBMRegressor(\n",
    "    boosting_type=\"gbdt\",\n",
    "    num_iterations = 2500,\n",
    "    learning_rate = 0.05,\n",
    "    num_leaves=15,\n",
    "    tree_learner='feature',\n",
    "    max_depth =10,\n",
    "    min_data_in_leaf=7,\n",
    "    bagging_fraction = 1,\n",
    "    bagging_freq = 100,\n",
    "    reg_sqrt='True',\n",
    "    metric ='rmse',\n",
    "    feature_fraction = 0.6,\n",
    "    random_state=42)\n",
    "\n",
    "model.fit(X_train,y_train) \n",
    "\n",
    "\n",
    "preds = model.predict(X_test)\n",
    "rmse_lgb = mean_squared_error(y_test, preds,squared = False)\n",
    "print(\" RMSE: %f\" % (rmse_lgb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predict Values for Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_raw = model.predict(filled_test.drop(['index'], axis=1))\n",
    "preds = predicted_raw.apply(round_to_nearest_hundred).to_numpy()\n",
    "preds_to_csv(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
